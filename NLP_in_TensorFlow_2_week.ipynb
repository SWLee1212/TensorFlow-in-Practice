{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_in_TensorFlow_2_week.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SWLee1212/TensorFlow-in-Practice/blob/master/NLP_in_TensorFlow_2_week.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxEiqzS7v2NC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7578366-cb43-49fe-9725-5acd46d3de95"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP8yGth6v9ON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "7be031f9-2616-41ed-c7e4-2903ce1227d1"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgTakrYQwL_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAkag6tcxL_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data, test_data = imdb['train'], imdb['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1HtjfxBxZEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "7fcd5095-1fbc-4815-a43c-3d898e44515b"
      },
      "source": [
        "info"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=0.1.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    urls=['http://ai.stanford.edu/~amaas/data/sentiment/'],\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQOuiJXgxZ9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
        "\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(str(s.numpy()))\n",
        "  training_labels.append(l.numpy())\n",
        "\n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(str(s.numpy()))\n",
        "  testing_labels.append(l.numpy())  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpplZFQn1Knh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f445ef94-f8dc-4030-8792-d736a04d89ac"
      },
      "source": [
        "sentence = tf.constant(\"This movie could be an all-time hit!\")\n",
        "label = tf.constant(1)\n",
        "# we can print tensors directly without creating \n",
        "# tf.Session() objects, thanks to eager execution\n",
        "print(sentence)\n",
        "print(label)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This movie could be an all-time hit!', shape=(), dtype=string)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va8D7Knx1zbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3iPk1Sv3IcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr5VnyCH3hBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a19cf68a-9820-4698-bcc9-17023523284f"
      },
      "source": [
        "print(sequences[0])\n",
        "print(len(padded[0]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[59, 11, 216, 12, 20, 22, 284, 101, 65, 5411, 93, 73, 1, 19, 164, 10, 14, 52, 138, 24, 87, 5, 2, 490, 115, 1315, 42, 2896, 422, 5, 65, 44, 7, 118, 19, 87, 164, 484, 51, 82, 136, 11, 196, 2, 20, 14, 1884, 9, 103, 2846, 5, 2368, 1976, 17, 1, 8, 8, 103, 24, 1, 6034, 1223, 24, 1, 654, 1963, 106, 36, 1, 145, 3149, 17, 1, 1877, 1223, 357, 3, 34, 518, 39, 357, 64, 8, 8, 1223, 4592, 2122, 3, 8986, 1496, 45, 285, 434, 12, 99, 29, 33, 1725, 3, 34, 99, 43, 29, 80, 156, 19, 11, 174, 105, 37, 1, 1, 415, 55, 24, 269, 6, 29, 654, 4, 54, 873, 234, 3, 11, 455, 90, 16, 10, 5682, 6665, 7, 6588, 3, 624, 19, 56, 14, 2763, 3, 4, 2368, 2, 2249, 5, 2, 108, 463, 627, 1, 22, 131, 1, 2, 115, 14, 54, 737, 3, 19, 37, 51, 11, 2436, 98, 26, 4413, 3, 5199, 6, 5618, 40, 2521, 243, 1092, 5619, 1036, 6, 728, 98, 2461, 296, 11, 105, 2, 106, 925, 33, 254, 336, 64, 856, 28, 78, 624, 3, 1316, 19, 34, 2498, 10, 5620, 2, 1, 5, 2, 915, 1223, 9, 13, 565, 1287, 9, 715, 3, 56, 2392, 22, 7141, 17, 2, 899, 11, 428, 1636, 32, 40, 16, 13, 11, 428, 1636, 32, 90, 16, 114, 142, 4, 503, 3, 16, 1, 40, 11, 428, 10, 31, 8, 8, 9, 2, 133, 11, 455, 10, 3, 64, 392, 10, 8, 8, 107, 46, 16, 2, 137, 121, 1, 1, 1145, 38, 2, 6035, 1, 7142, 620, 234]\n",
            "120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFtpxVZ24Pji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "09e70a1e-23d4-4698-866d-001b064ede35"
      },
      "source": [
        "print(testing_sequences[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[59, 402, 298, 2, 18, 164, 458, 207, 3, 51, 11, 182, 6, 136, 45, 10, 7, 2, 67, 153, 13, 93, 12, 18, 315, 4273, 14, 2, 1, 1, 1061, 234, 439, 163, 19, 13, 24, 62, 149, 719, 2183, 783, 1, 1, 3, 50, 5, 135, 1082, 1458, 32, 207, 9, 2, 18, 1, 7, 416, 54, 1031, 402, 111, 4, 169, 2669, 27, 93, 3, 78, 145, 62, 152, 12, 7, 239, 5, 189, 18, 565, 406, 68, 52, 293, 9, 209, 101, 3, 84, 9, 3308, 100, 11, 2154, 92, 5, 23, 31, 216, 1, 1, 2, 18, 3, 242, 198, 149, 108, 64, 141, 16, 35, 473, 31, 319, 2168, 136, 7, 23, 130, 107, 46, 16, 2, 159, 504, 596, 46, 38, 1, 95, 1001, 69, 410, 2, 52, 157, 1, 77, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdMLiGQL8bM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f33f677b-7064-4370-f677-8a76484321f4"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(padded[1]))\n",
        "print(training_sentences[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b this is the most depressing film i have ever seen i first saw it as a child and even thinking about it now really <OOV> me i know it was set in a time when life was hard and i know these people were poor and the <OOV> were vital yes i get all that what i find hard to take is i can't remember one single light moment in the entire film maybe it was true to life i don't know i'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and i know that every film can't have a happy ending but as a family film it is\n",
            "b\"This is the most depressing film I have ever seen. I first saw it as a child and even thinking about it now really upsets me. I know it was set in a time when life was hard and I know these people were poor and the crops were vital. Yes, I get all that. What I find hard to take is I can't remember one single light moment in the entire film. Maybe it was true to life, I don't know. I'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and I know that every film can't have a happy ending but as a family film it is dire in my opinion.<br /><br />I wouldn't recommend it to anyone who wants to be entertained by a film. I can't stress enough how this film affected me as a child. I was talking about it recently and all the sad memories came flooding back. I think it would have all but the heartless reaching for the Prozac.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lHvyKIo97--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c96d9ac-2daf-42a4-d13e-e05d1cc99f9e"
      },
      "source": [
        "print(reverse_word_index.get('?'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXvA7jLTOWf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "381aa455-29ac-46df-8486-31800b15f3f4"
      },
      "source": [
        "def decode_review_test(text):\n",
        "    return ' '.join([reverse_word_index.get(i) for i in text])\n",
        "\n",
        "print(decode_review_test(padded[1]))\n",
        "print(decode_review(padded[1]))\n",
        "\n",
        "print(decode_review_test(padded[1]) == decode_review(padded[1]) )"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b this is the most depressing film i have ever seen i first saw it as a child and even thinking about it now really <OOV> me i know it was set in a time when life was hard and i know these people were poor and the <OOV> were vital yes i get all that what i find hard to take is i can't remember one single light moment in the entire film maybe it was true to life i don't know i'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and i know that every film can't have a happy ending but as a family film it is\n",
            "b this is the most depressing film i have ever seen i first saw it as a child and even thinking about it now really <OOV> me i know it was set in a time when life was hard and i know these people were poor and the <OOV> were vital yes i get all that what i find hard to take is i can't remember one single light moment in the entire film maybe it was true to life i don't know i'm quite sure the acting was top notch and the direction and quality of filming etc etc was wonderful and i know that every film can't have a happy ending but as a family film it is\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEegZQ2dPy0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "835fd084-fb7a-4666-bcf8-36d7ae02f866"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 11526     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 171,533\n",
            "Trainable params: 171,533\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3e7wfTS5NN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "4cd6b8ca-ff3d-4350-da70-2ab25fd8d0e0"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 5s 215us/sample - loss: 0.5164 - acc: 0.7176 - val_loss: 0.3575 - val_acc: 0.8419\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 5s 189us/sample - loss: 0.2486 - acc: 0.9053 - val_loss: 0.3663 - val_acc: 0.8396\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 5s 188us/sample - loss: 0.0978 - acc: 0.9750 - val_loss: 0.4660 - val_acc: 0.8205\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 5s 193us/sample - loss: 0.0280 - acc: 0.9960 - val_loss: 0.5173 - val_acc: 0.8266\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 5s 188us/sample - loss: 0.0110 - acc: 0.9986 - val_loss: 0.5672 - val_acc: 0.8282\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 5s 190us/sample - loss: 0.0053 - acc: 0.9993 - val_loss: 0.6168 - val_acc: 0.8257\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 5s 187us/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.6671 - val_acc: 0.8252\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 5s 189us/sample - loss: 5.8953e-04 - acc: 1.0000 - val_loss: 0.7079 - val_acc: 0.8257\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 5s 189us/sample - loss: 3.1593e-04 - acc: 1.0000 - val_loss: 0.7477 - val_acc: 0.8256\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 5s 189us/sample - loss: 1.8327e-04 - acc: 1.0000 - val_loss: 0.7805 - val_acc: 0.8264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc51272c940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UyM_o6tS8ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b141dc2-4af2-4751-e8bf-112cb6acfe85"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oue2iduTT5V8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd2eaf75-0683-4f1c-e8e7-7378e4736c31"
      },
      "source": [
        "len(model.layers)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxNr0RKNT9z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rr9CNkoUNXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl15_l9LUOpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "54e1ee6c-430a-447d-a797-5f061a1c9acc"
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11], [], [1430], [968], [4], [1537], [1537], [4738], [], [790], [2015], [11], [2922], [2189], [], [790], [2015], [11], [579], [], [11], [579], [], [4], [1783], [4], [4508], [11], [2922], [1277], [], [], [2015], [1005], [2922], [968], [579], [790], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNT2inKBXpfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cce57190-9f44-46d9-b93d-5d0fb63ec983"
      },
      "source": [
        "testing_padded.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8-4SXdMXveM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "5d3721ea-415f-4ecb-c04d-5f278d866768"
      },
      "source": [
        "np.array(sequence)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([11]), list([]), list([1430]), list([968]), list([4]),\n",
              "       list([1537]), list([1537]), list([4738]), list([]), list([790]),\n",
              "       list([2015]), list([11]), list([2922]), list([2189]), list([]),\n",
              "       list([790]), list([2015]), list([11]), list([579]), list([]),\n",
              "       list([11]), list([579]), list([]), list([4]), list([1783]),\n",
              "       list([4]), list([4508]), list([11]), list([2922]), list([1277]),\n",
              "       list([]), list([]), list([2015]), list([1005]), list([2922]),\n",
              "       list([968]), list([579]), list([790]), list([])], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0tnULzhYA5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}